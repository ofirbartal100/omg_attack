ckpt:
#vm_ckpt: [/home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10/epoch=256-step=99999.ckpt,/home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10/epoch=153-step=59999-v1.ckpt,/home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10/epoch=102-step=39999-v1.ckpt,/home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10/epoch=205-step=79999-v1.ckpt,/home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10/epoch=205-step=79999.ckpt]
vm_ckpt: /home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10_5%
data_root: /disk2/ofirb/dabs/data
gpus: [7]
system: ViewmakerTransferSystemDiversity
debug: false

viewmaker:
  config_path: /home/shahafg/shared/Apple/dabs/exp/models/vm_disc_cifar10/vm_config.yaml
#  additive_budget: 0.1
  reroll_prob: 0.5
  aug_proba: 0.7

exp:
  base_dir: /home/shahafg/shared/Apple/dabs/exp
  name: resnet_flowlp valaug vm_disc_5% cifar10 -> cifar10 5% | n-th 0.5 | p0.7

trainer:
  weights_summary: top
  seed: 0
  val_check_interval: 1.0
  limit_val_batches: 1.0
  precision: 32  # set to 16 for O1 mixed precision
  max_epochs: 200
  gradient_clip_val: 0


### this is following the cutout paper
### https://arxiv.org/pdf/1708.04552.pdf
optim:
  name: sgd
  lr: 0.1
  weight_decay: 5e-4
  momentum: 0.9  # only used for momentum-based optimizers
  nesterov: true
  lr_scheduler:
    gamma: 0.2
    milestones: [60, 120, 160]

defaults:
  - dataset: cifar10_small
  # - dataset: librispeech_transfer
  - model: resnet18_flowlp  # used to set default model when no ckpt is passed in


dataset:
  batch_size: 128
  low_data: 0.05
