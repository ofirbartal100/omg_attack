{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import flatten_dict\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from dabs.src.systems import viewmaker, viewmaker_original\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as image\n",
    "import torchvision\n",
    "from IPython import display\n",
    "from viewmaker.src.systems.image_systems.utils import heatmap_of_view_effect\n",
    "from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show(imgs,**fig_kwr):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False,**fig_kwr)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m31367 train examples, 7842 val examples\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrafficViewMaker(\n",
       "  (model): TrafficModel(\n",
       "    (embed_modules): ModuleList()\n",
       "    (traffic_model): Net(\n",
       "      (conv1): Conv2d(3, 100, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(100, 150, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn2): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(150, 250, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv_drop): Dropout2d(p=0.5, inplace=False)\n",
       "      (fc1): Linear(in_features=1000, out_features=350, bias=True)\n",
       "      (fc2): Linear(in_features=350, out_features=43, bias=True)\n",
       "      (localization): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (fc_loc): Sequential(\n",
       "        (0): Linear(in_features=160, out_features=32, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (viewmaker): Viewmaker(\n",
       "    (act): ReLU()\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "      (conv2d): Conv2d(4, 32, kernel_size=(9, 9), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv3): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (in3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (res1): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(129, 129, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(129, 129, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res2): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(130, 130, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(130, 130, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res3): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(131, 131, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(131, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(131, 131, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(131, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res4): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res5): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (deconv1): UpsampleConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(131, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (deconv2): UpsampleConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (deconv3): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "      (conv2d): Conv2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (memory_bank): MemoryBank()\n",
       "  (memory_bank_labels): MemoryBank()\n",
       "  (disc): TinyP2PDiscriminator(\n",
       "    (conv_block1): DescConvBlock(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm): Identity()\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (conv_block2): DescConvBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (conv_block3): DescConvBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (conv_block4): DescConvBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (final_conv): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = OmegaConf.load('/workspace/dabs/conf/traffic.yaml')\n",
    "config.debug = True\n",
    "config.dataset = OmegaConf.load('/workspace/dabs/conf/dataset/traffic_sign_small.yaml')\n",
    "config.model = OmegaConf.load('/workspace/dabs/conf/model/traffic_model.yaml')\n",
    "\n",
    "config.dataset.batch_size = 64\n",
    "\n",
    "pl.seed_everything(config.trainer.seed)\n",
    "\n",
    "system = viewmaker_original.TrafficViewMaker(config)\n",
    "system.setup('')\n",
    "system.load_state_dict(torch.load('/workspace/dabs/exp/models/traffic_gan/presentation.ckpt')['state_dict'],strict=False)\n",
    "\n",
    "system.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"PosixPath\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dabs/traffic_views_check_performance.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img)\u001b[39m.\u001b[39mconvert(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     image \u001b[39m=\u001b[39m transform(image)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2962\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2960\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 2962\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[1;32m   2964\u001b[0m preinit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dabs/traffic_views_check_performance.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         labels\u001b[39m.\u001b[39mappend(\u001b[39mint\u001b[39m(\u001b[39mstr\u001b[39m(img)\u001b[39m.\u001b[39mreplace(adv_dir,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)[:\u001b[39m5\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39mError in \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m img)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(labels))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64616273222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6f666972622d64616273227d7d/workspace/dabs/traffic_views_check_performance.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m X_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"PosixPath\") to str"
     ]
    }
   ],
   "source": [
    "adv_dir = \"/workspace/dabs/data/natural_images/traffic_sign/GTSRB/Validation_Adversarial_v0/Images/\"\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "result = list(Path(adv_dir).rglob(\"*.ppm\"))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [transforms.Resize((32, 32)),\n",
    "             transforms.CenterCrop((32, 32)),\n",
    "             transforms.ToTensor()]\n",
    "        )\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for img in result:\n",
    "    try:\n",
    "        image = Image.open(img).convert(mode='RGB')\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(int(str(img).replace(adv_dir,'')[:5]))\n",
    "    except:\n",
    "        print(\"Error in \" + img)\n",
    "\n",
    "labels = torch.from_numpy(np.array(labels))\n",
    "X_test = torch.stack(data)\n",
    "\n",
    "pred=system.predict(X_test).squeeze()\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',((pred==labels).sum()/len(labels))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on augmentations of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018553495407104492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7808,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb3867edde74b7689bc0249d304eedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(123)\n",
    "\n",
    "no_transform = transforms.Compose(\n",
    "            [transforms.Resize((32, 32)),\n",
    "             transforms.CenterCrop((32, 32)),\n",
    "             transforms.ToTensor()]\n",
    "        )\n",
    "\n",
    "# Resize, normalize and jitter image brightness\n",
    "data_jitter_brightness = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    # transforms.ColorJitter(brightness=-5),\n",
    "    transforms.ColorJitter(brightness=5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and jitter image saturation\n",
    "data_jitter_saturation = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ColorJitter(saturation=5),\n",
    "    # transforms.ColorJitter(saturation=-5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and jitter image contrast\n",
    "data_jitter_contrast = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ColorJitter(contrast=5),\n",
    "    # transforms.ColorJitter(contrast=-5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and jitter image hues\n",
    "data_jitter_hue = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ColorJitter(hue=0.4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and rotate image\n",
    "data_rotate = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and flip image horizontally and vertically\n",
    "data_hvflip = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.RandomVerticalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and flip image horizontally\n",
    "data_hflip = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and flip image vertically\n",
    "data_vflip = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomVerticalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and shear image\n",
    "data_shear = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomAffine(degrees = 15,shear=2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and translate image\n",
    "data_translate = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomAffine(degrees = 15,translate=(0.1,0.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and crop image \n",
    "data_center = transforms.Compose([\n",
    "\ttransforms.Resize((36, 36)),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and convert image to grayscale\n",
    "data_grayscale = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and convert image to grayscale\n",
    "sharpness = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_candidates = [no_transform,data_jitter_saturation,data_jitter_contrast,data_jitter_hue,data_rotate,data_hvflip,data_hflip,data_vflip,data_shear,data_translate,data_center,data_grayscale,sharpness]\n",
    "\n",
    "# for transform in transform_candidates:\n",
    "\n",
    "# data = []\n",
    "labels = []\n",
    "for img in tqdm(result):\n",
    "    try:\n",
    "        label = int(str(img).replace(adv_dir,'')[:5])\n",
    "        image = Image.open(img).convert(mode='RGB')\n",
    "        x = torch.stack([ transform(image) for transform in transform_candidates])\n",
    "        pred=system.predict(x).squeeze()\n",
    "        labels.append(torch.hstack([torch.Tensor([label]),pred]).numpy())\n",
    "    except:\n",
    "        print(\"Error in \" + img)\n",
    "\n",
    "results_df = pd.DataFrame(data=np.array(labels),columns=['original','no_transform','data_jitter_saturation','data_jitter_contrast','data_jitter_hue','data_rotate','data_hvflip','data_hflip','data_vflip','data_shear','data_translate','data_center','data_grayscale','sharpness'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['original','no_transform','data_jitter_saturation','data_jitter_contrast','data_jitter_hue','data_rotate','data_hvflip','data_hflip','data_vflip','data_shear','data_translate','data_center','data_grayscale','sharpness']\n",
    "\n",
    "d = {}\n",
    "for col in columns[1:]:\n",
    "    d[f'original_vs_{col}'] = [(results_df['original'] == results_df[col]).mean()]\n",
    "\n",
    "for col in columns[2:]:\n",
    "    d[f'no_transform_vs_{col}'] = [(results_df['no_transform'] == results_df[col]).mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original_vs_no_transform</th>\n",
       "      <td>0.300077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_jitter_saturation</th>\n",
       "      <td>0.318648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_jitter_contrast</th>\n",
       "      <td>0.311475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_jitter_hue</th>\n",
       "      <td>0.284708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_rotate</th>\n",
       "      <td>0.301486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_hvflip</th>\n",
       "      <td>0.299821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_hflip</th>\n",
       "      <td>0.269211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_vflip</th>\n",
       "      <td>0.251793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_shear</th>\n",
       "      <td>0.301358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_translate</th>\n",
       "      <td>0.309298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_center</th>\n",
       "      <td>0.288806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_grayscale</th>\n",
       "      <td>0.277536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_sharpness</th>\n",
       "      <td>0.294185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_jitter_saturation</th>\n",
       "      <td>0.853740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_jitter_contrast</th>\n",
       "      <td>0.781762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_jitter_hue</th>\n",
       "      <td>0.848105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_rotate</th>\n",
       "      <td>0.862065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_hvflip</th>\n",
       "      <td>0.694032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_hflip</th>\n",
       "      <td>0.380379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_vflip</th>\n",
       "      <td>0.357070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_shear</th>\n",
       "      <td>0.859503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_translate</th>\n",
       "      <td>0.768186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_center</th>\n",
       "      <td>0.856814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_grayscale</th>\n",
       "      <td>0.844390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_sharpness</th>\n",
       "      <td>0.903945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0\n",
       "original_vs_no_transform                0.300077\n",
       "original_vs_data_jitter_saturation      0.318648\n",
       "original_vs_data_jitter_contrast        0.311475\n",
       "original_vs_data_jitter_hue             0.284708\n",
       "original_vs_data_rotate                 0.301486\n",
       "original_vs_data_hvflip                 0.299821\n",
       "original_vs_data_hflip                  0.269211\n",
       "original_vs_data_vflip                  0.251793\n",
       "original_vs_data_shear                  0.301358\n",
       "original_vs_data_translate              0.309298\n",
       "original_vs_data_center                 0.288806\n",
       "original_vs_data_grayscale              0.277536\n",
       "original_vs_sharpness                   0.294185\n",
       "no_transform_vs_data_jitter_saturation  0.853740\n",
       "no_transform_vs_data_jitter_contrast    0.781762\n",
       "no_transform_vs_data_jitter_hue         0.848105\n",
       "no_transform_vs_data_rotate             0.862065\n",
       "no_transform_vs_data_hvflip             0.694032\n",
       "no_transform_vs_data_hflip              0.380379\n",
       "no_transform_vs_data_vflip              0.357070\n",
       "no_transform_vs_data_shear              0.859503\n",
       "no_transform_vs_data_translate          0.768186\n",
       "no_transform_vs_data_center             0.856814\n",
       "no_transform_vs_data_grayscale          0.844390\n",
       "no_transform_vs_sharpness               0.903945"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(d).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 100, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(100, 150, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(150, 250, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1000, out_features=350, bias=True)\n",
       "  (fc2): Linear(in_features=350, out_features=43, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from gtsrb_pytorch.model import Net\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from gtsrb_pytorch.data import *\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "state_dict = torch.load('/workspace/gtsrb_pytorch/model/model_40.pth')\n",
    "model = Net()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "\tuse_gpu = False\n",
    "\tprint(\"Using CPU\")\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# val_data_path = '/workspace/dabs/data/adv_data/traffic_sign/07_01_2023/traffic_budget_budget=0.005/val'\n",
    "val_data_path = '/workspace/dabs/data/adv_data/traffic_sign/FGSM/val'\n",
    "# val_data_path = '/workspace/gtsrb_pytorch/data/val_images'\n",
    "# Apply data transformations on the training images to augment dataset\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,val_loader):\n",
    "    correct = 0\n",
    "    for data, target in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            if use_gpu:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model.forward_original(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    acc = correct / len(val_loader.dataset)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:01<00:00, 132.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_transforms: acc 6.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 111.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_brightness: acc 5.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 109.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_saturation: acc 6.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 96.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_contrast: acc 5.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 94.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_hue: acc 6.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 116.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_rotate: acc 6.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 121.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_hvflip: acc 6.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:01<00:00, 122.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_hflip: acc 5.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:01<00:00, 123.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_vflip: acc 5.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 115.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_shear: acc 6.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 114.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_translate: acc 6.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:01<00:00, 122.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_center: acc 6.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:02<00:00, 118.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_grayscale: acc 6.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traffic_transforms = {'data_transforms': data_transforms,'data_jitter_brightness': data_jitter_brightness,'data_jitter_saturation': data_jitter_saturation,'data_jitter_contrast': data_jitter_contrast,\n",
    "'data_jitter_hue': data_jitter_hue,'data_rotate': data_rotate,'data_hvflip': data_hvflip,'data_hflip': data_hflip,'data_vflip': data_vflip,'data_shear': data_shear,'data_translate': data_translate,\n",
    "'data_center': data_center,'data_grayscale': data_grayscale}\n",
    "\n",
    "results = dict()\n",
    "for k in traffic_transforms.keys():\n",
    "    t = traffic_transforms[k]\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(val_data_path,transform=t,is_valid_file=lambda s: 'view' in s),\n",
    "    batch_size=32, shuffle=False, num_workers=4, pin_memory=use_gpu)\n",
    "    acc = validate(model,val_loader)\n",
    "    results[k] = acc\n",
    "    print(f'{k}: acc {acc*100.0:0.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd_result = pd.DataFrame.from_dict({k: [results[k].item()] for k in results.keys()})\n",
    "sns.barplot(pd_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_adv ASR = 93.82684417068958% ; mean additional transform ASR = 94.03389689202109%\n"
     ]
    }
   ],
   "source": [
    "raw_data_key = 'data_transforms'\n",
    "raw_adversarial = results[raw_data_key].item()\n",
    "w_transform = np.mean([results[k].item() for k in results.keys() if k!=raw_data_key])\n",
    "print(f'raw_adv ASR = {(1-raw_adversarial)*100.}% ; mean additional transform ASR = {(1-w_transform)*100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
