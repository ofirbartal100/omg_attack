{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import flatten_dict\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from dabs.src.systems import viewmaker, viewmaker_original\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as image\n",
    "import torchvision\n",
    "from IPython import display\n",
    "from viewmaker.src.systems.image_systems.utils import heatmap_of_view_effect\n",
    "from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show(imgs,**fig_kwr):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False,**fig_kwr)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m31367 train examples, 7842 val examples\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrafficViewMaker(\n",
       "  (model): TrafficModel(\n",
       "    (embed_modules): ModuleList()\n",
       "    (traffic_model): Net(\n",
       "      (conv1): Conv2d(3, 100, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(100, 150, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn2): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(150, 250, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv_drop): Dropout2d(p=0.5, inplace=False)\n",
       "      (fc1): Linear(in_features=1000, out_features=350, bias=True)\n",
       "      (fc2): Linear(in_features=350, out_features=43, bias=True)\n",
       "      (localization): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (fc_loc): Sequential(\n",
       "        (0): Linear(in_features=160, out_features=32, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (viewmaker): Viewmaker(\n",
       "    (act): ReLU()\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "      (conv2d): Conv2d(4, 32, kernel_size=(9, 9), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv3): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (in3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (res1): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(129, 129, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(129, 129, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res2): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(130, 130, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(130, 130, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res3): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(131, 131, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(131, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(131, 131, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(131, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res4): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (res5): ResidualBlock(\n",
       "      (conv1): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in1): InstanceNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv2): ConvLayer(\n",
       "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1))\n",
       "      )\n",
       "      (in2): InstanceNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (deconv1): UpsampleConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(131, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (deconv2): UpsampleConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (deconv3): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "      (conv2d): Conv2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (memory_bank): MemoryBank()\n",
       "  (memory_bank_labels): MemoryBank()\n",
       "  (disc): TinyP2PDiscriminator(\n",
       "    (conv_block1): DescConvBlock(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm): Identity()\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (conv_block2): DescConvBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (conv_block3): DescConvBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (conv_block4): DescConvBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (final_conv): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = OmegaConf.load('/workspace/dabs/conf/traffic.yaml')\n",
    "config.debug = True\n",
    "config.dataset = OmegaConf.load('/workspace/dabs/conf/dataset/traffic_sign_small.yaml')\n",
    "config.model = OmegaConf.load('/workspace/dabs/conf/model/traffic_model.yaml')\n",
    "\n",
    "config.dataset.batch_size = 64\n",
    "\n",
    "pl.seed_everything(config.trainer.seed)\n",
    "\n",
    "system = viewmaker_original.TrafficViewMaker(config)\n",
    "system.setup('')\n",
    "system.load_state_dict(torch.load('/workspace/dabs/exp/models/traffic_gan/presentation.ckpt')['state_dict'],strict=False)\n",
    "\n",
    "system.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data accuracy:  tensor(30.0077)\n"
     ]
    }
   ],
   "source": [
    "adv_dir = \"/workspace/dabs/data/natural_images/traffic_sign/GTSRB/Validation_Adversarial_v0/Images/\"\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "result = list(Path(adv_dir).rglob(\"*.ppm\"))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [transforms.Resize((32, 32)),\n",
    "             transforms.CenterCrop((32, 32)),\n",
    "             transforms.ToTensor()]\n",
    "        )\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for img in result:\n",
    "    try:\n",
    "        image = Image.open(img).convert(mode='RGB')\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(int(str(img).replace(adv_dir,'')[:5]))\n",
    "    except:\n",
    "        print(\"Error in \" + img)\n",
    "\n",
    "labels = torch.from_numpy(np.array(labels))\n",
    "X_test = torch.stack(data)\n",
    "\n",
    "pred=system.predict(X_test).squeeze()\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',((pred==labels).sum()/len(labels))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on augmentations of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018553495407104492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7808,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb3867edde74b7689bc0249d304eedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(123)\n",
    "\n",
    "no_transform = transforms.Compose(\n",
    "            [transforms.Resize((32, 32)),\n",
    "             transforms.CenterCrop((32, 32)),\n",
    "             transforms.ToTensor()]\n",
    "        )\n",
    "\n",
    "# Resize, normalize and jitter image brightness\n",
    "data_jitter_brightness = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    # transforms.ColorJitter(brightness=-5),\n",
    "    transforms.ColorJitter(brightness=5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and jitter image saturation\n",
    "data_jitter_saturation = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ColorJitter(saturation=5),\n",
    "    # transforms.ColorJitter(saturation=-5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and jitter image contrast\n",
    "data_jitter_contrast = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ColorJitter(contrast=5),\n",
    "    # transforms.ColorJitter(contrast=-5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and jitter image hues\n",
    "data_jitter_hue = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ColorJitter(hue=0.4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and rotate image\n",
    "data_rotate = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and flip image horizontally and vertically\n",
    "data_hvflip = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.RandomVerticalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and flip image horizontally\n",
    "data_hflip = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and flip image vertically\n",
    "data_vflip = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomVerticalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and shear image\n",
    "data_shear = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomAffine(degrees = 15,shear=2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and translate image\n",
    "data_translate = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomAffine(degrees = 15,translate=(0.1,0.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and crop image \n",
    "data_center = transforms.Compose([\n",
    "\ttransforms.Resize((36, 36)),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and convert image to grayscale\n",
    "data_grayscale = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Resize, normalize and convert image to grayscale\n",
    "sharpness = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_candidates = [no_transform,data_jitter_saturation,data_jitter_contrast,data_jitter_hue,data_rotate,data_hvflip,data_hflip,data_vflip,data_shear,data_translate,data_center,data_grayscale,sharpness]\n",
    "\n",
    "# for transform in transform_candidates:\n",
    "\n",
    "# data = []\n",
    "labels = []\n",
    "for img in tqdm(result):\n",
    "    try:\n",
    "        label = int(str(img).replace(adv_dir,'')[:5])\n",
    "        image = Image.open(img).convert(mode='RGB')\n",
    "        x = torch.stack([ transform(image) for transform in transform_candidates])\n",
    "        pred=system.predict(x).squeeze()\n",
    "        labels.append(torch.hstack([torch.Tensor([label]),pred]).numpy())\n",
    "    except:\n",
    "        print(\"Error in \" + img)\n",
    "\n",
    "results_df = pd.DataFrame(data=np.array(labels),columns=['original','no_transform','data_jitter_saturation','data_jitter_contrast','data_jitter_hue','data_rotate','data_hvflip','data_hflip','data_vflip','data_shear','data_translate','data_center','data_grayscale','sharpness'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['original','no_transform','data_jitter_saturation','data_jitter_contrast','data_jitter_hue','data_rotate','data_hvflip','data_hflip','data_vflip','data_shear','data_translate','data_center','data_grayscale','sharpness']\n",
    "\n",
    "d = {}\n",
    "for col in columns[1:]:\n",
    "    d[f'original_vs_{col}'] = [(results_df['original'] == results_df[col]).mean()]\n",
    "\n",
    "for col in columns[2:]:\n",
    "    d[f'no_transform_vs_{col}'] = [(results_df['no_transform'] == results_df[col]).mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original_vs_no_transform</th>\n",
       "      <td>0.300077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_jitter_saturation</th>\n",
       "      <td>0.318648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_jitter_contrast</th>\n",
       "      <td>0.311475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_jitter_hue</th>\n",
       "      <td>0.284708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_rotate</th>\n",
       "      <td>0.301486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_hvflip</th>\n",
       "      <td>0.299821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_hflip</th>\n",
       "      <td>0.269211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_vflip</th>\n",
       "      <td>0.251793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_shear</th>\n",
       "      <td>0.301358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_translate</th>\n",
       "      <td>0.309298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_center</th>\n",
       "      <td>0.288806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_data_grayscale</th>\n",
       "      <td>0.277536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_vs_sharpness</th>\n",
       "      <td>0.294185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_jitter_saturation</th>\n",
       "      <td>0.853740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_jitter_contrast</th>\n",
       "      <td>0.781762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_jitter_hue</th>\n",
       "      <td>0.848105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_rotate</th>\n",
       "      <td>0.862065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_hvflip</th>\n",
       "      <td>0.694032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_hflip</th>\n",
       "      <td>0.380379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_vflip</th>\n",
       "      <td>0.357070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_shear</th>\n",
       "      <td>0.859503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_translate</th>\n",
       "      <td>0.768186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_center</th>\n",
       "      <td>0.856814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_data_grayscale</th>\n",
       "      <td>0.844390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_transform_vs_sharpness</th>\n",
       "      <td>0.903945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0\n",
       "original_vs_no_transform                0.300077\n",
       "original_vs_data_jitter_saturation      0.318648\n",
       "original_vs_data_jitter_contrast        0.311475\n",
       "original_vs_data_jitter_hue             0.284708\n",
       "original_vs_data_rotate                 0.301486\n",
       "original_vs_data_hvflip                 0.299821\n",
       "original_vs_data_hflip                  0.269211\n",
       "original_vs_data_vflip                  0.251793\n",
       "original_vs_data_shear                  0.301358\n",
       "original_vs_data_translate              0.309298\n",
       "original_vs_data_center                 0.288806\n",
       "original_vs_data_grayscale              0.277536\n",
       "original_vs_sharpness                   0.294185\n",
       "no_transform_vs_data_jitter_saturation  0.853740\n",
       "no_transform_vs_data_jitter_contrast    0.781762\n",
       "no_transform_vs_data_jitter_hue         0.848105\n",
       "no_transform_vs_data_rotate             0.862065\n",
       "no_transform_vs_data_hvflip             0.694032\n",
       "no_transform_vs_data_hflip              0.380379\n",
       "no_transform_vs_data_vflip              0.357070\n",
       "no_transform_vs_data_shear              0.859503\n",
       "no_transform_vs_data_translate          0.768186\n",
       "no_transform_vs_data_center             0.856814\n",
       "no_transform_vs_data_grayscale          0.844390\n",
       "no_transform_vs_sharpness               0.903945"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(d).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 100, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(100, 150, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(150, 250, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1000, out_features=350, bias=True)\n",
       "  (fc2): Linear(in_features=350, out_features=43, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from gtsrb_pytorch.model import Net\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from gtsrb_pytorch.data import *\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "state_dict = torch.load('/workspace/gtsrb_pytorch/model/model_40.pth')\n",
    "model = Net()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "\tuse_gpu = False\n",
    "\tprint(\"Using CPU\")\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_data_path = '/workspace/dabs/data/adv_data/traffic_sign/07_01_2023/traffic_budget_budget=0.005/val'\n",
    "# val_data_path = '/workspace/gtsrb_pytorch/data/val_images'\n",
    "# Apply data transformations on the training images to augment dataset\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,val_loader):\n",
    "    correct = 0\n",
    "    for data, target in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            if use_gpu:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model.forward_original(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    acc = correct / len(val_loader.dataset)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:05<00:00, 92.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_transforms: acc 40.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 146.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_brightness: acc 37.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 138.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_saturation: acc 41.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:04<00:00, 119.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_contrast: acc 41.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:04<00:00, 112.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_jitter_hue: acc 39.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 150.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_rotate: acc 40.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 156.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_hvflip: acc 40.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 162.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_hflip: acc 30.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 162.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_vflip: acc 29.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 147.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_shear: acc 40.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 144.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_translate: acc 40.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 152.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_center: acc 38.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:03<00:00, 151.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_grayscale: acc 38.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traffic_transforms = {'data_transforms': data_transforms,'data_jitter_brightness': data_jitter_brightness,'data_jitter_saturation': data_jitter_saturation,'data_jitter_contrast': data_jitter_contrast,\n",
    "'data_jitter_hue': data_jitter_hue,'data_rotate': data_rotate,'data_hvflip': data_hvflip,'data_hflip': data_hflip,'data_vflip': data_vflip,'data_shear': data_shear,'data_translate': data_translate,\n",
    "'data_center': data_center,'data_grayscale': data_grayscale}\n",
    "\n",
    "results = dict()\n",
    "for k in traffic_transforms.keys():\n",
    "    t = traffic_transforms[k]\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(val_data_path,transform=t,is_valid_file=lambda s: 'view' in s),\n",
    "    batch_size=32, shuffle=False, num_workers=4, pin_memory=use_gpu)\n",
    "    acc = validate(model,val_loader)\n",
    "    results[k] = acc\n",
    "    print(f'{k}: acc {acc*100.0:0.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd_result = pd.DataFrame.from_dict({k: [results[k].item()] for k in results.keys()})\n",
    "sns.barplot(pd_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_adv ASR = 59.47105586528778% ; mean additional transform ASR = 61.8009311457475%\n"
     ]
    }
   ],
   "source": [
    "raw_data_key = 'data_transforms'\n",
    "raw_adversarial = results[raw_data_key].item()\n",
    "w_transform = np.mean([results[k].item() for k in results.keys() if k!=raw_data_key])\n",
    "print(f'raw_adv ASR = {(1-raw_adversarial)*100.}% ; mean additional transform ASR = {(1-w_transform)*100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
